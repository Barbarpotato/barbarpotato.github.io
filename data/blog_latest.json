[
  {
    "blog_id": "86f7440f-033f-4459-b0a5-09f74d7c34ba",
    "title": "Understanding Circuit Breakers in Software Engineering: From Traditional to Serverless",
    "short_description": "Imagine you’re using electricity at home, and a short circuit occurs. The circuit breaker in your electrical panel cuts the power to prevent a fire. In software, the concept is similar: it’s a design pattern that protects your system from repeated failures when calling external services",
    "timestamp": "2025-03-14 10:46:27",
    "description": "<h2>What Is a Circuit Breaker?</h2><p>Imagine you’re using electricity at home, and a short circuit occurs. The <em>circuit breaker</em> in your electrical panel cuts the power to prevent a fire. In software, the concept is similar: it’s a design pattern that protects your system from repeated failures when calling external services (APIs, databases, etc.).</p><h2>Main Purposes:</h2><h3><strong>Detect Failures</strong></h3><p>The first job of a <em>circuit breaker</em> is to act as a vigilant watchdog, constantly monitoring interactions between your application and external services like APIs, databases, or third-party systems. It keeps an eye on every request, tracking whether they succeed or fail based on specific criteria, such as receiving an error code (e.g., HTTP 500), timing out after a set duration (e.g., no response within 2 seconds), or encountering exceptions like network disconnections.</p><p>To do this effectively, the circuit breaker collects data over a defined window—perhaps the last 10 requests or the past 30 seconds—and calculates metrics like the total number of failures or the failure rate (e.g., 60% of calls failed). If these metrics cross a configurable threshold—say, five failures in a row or a 50% error rate—it recognizes that something’s wrong with the external service. This detection isn’t just about noticing a single hiccup; it’s about identifying patterns of unreliability that could harm your system if left unchecked. By catching these issues early, the circuit breaker ensures your application doesn’t blindly keep trying a service that’s clearly struggling.</p><h3><strong>Prevent Cascading Failures</strong></h3><p>Once a failure is detected, the circuit breaker steps in to stop a domino effect known as <em>cascading failures</em>, where one broken component drags down the entire system. Imagine an e-commerce app where the payment API is down: without a circuit breaker, every user request might hang, waiting for a timeout, piling up server resources, slowing the database, and eventually crashing the whole application.</p><p>In its Closed state, the circuit breaker allows calls to proceed, but as soon as failures hit the threshold, it flips to Open, cutting off all further attempts to contact the faulty service. This immediate halt prevents the problem from rippling through your system—your app stops wasting threads, memory, or CPU cycles on a hopeless task. Instead of letting a single point of failure—like a slow third-party API—overload your servers or exhaust connection pools, the circuit breaker isolates the issue, keeping the rest of your application stable and responsive. It’s like closing a floodgate to protect the town downstream from a burst dam.</p><h3><strong>Provide a Fallback Response</strong></h3><p>When the circuit breaker blocks calls in its Open state, it doesn’t just leave users hanging—it offers a fallback response to keep the system usable. This fallback is a preplanned alternative to the failed service’s output, designed to minimize disruption.</p><p>For example, if a weather API fails, the circuit breaker might return a cached forecast from an hour ago or a simple message like \"Weather data unavailable, try again later.\" In a payment system, it could redirect users to an alternative checkout method or log the attempt for later retry. The fallback doesn’t fix the root problem, but it ensures graceful degradation.</p><p>Your application keeps running in a limited capacity rather than crashing or showing cryptic errors. Crafting a good fallback requires understanding your use case: it might be static data, a default value, or even a call to a backup service. By providing this safety net, the circuit breaker maintains user trust and buys time for the external service to recover without sacrificing functionality entirely.</p><p><img src=\"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741949039277_Circuit-Breaker-Pattern.jpg\" alt=\"Circuit Breaker Pattern\" width=\"720px\"></p><h2>Overall Mechanism</h2><ol><li><strong>Closed</strong>: All calls are forwarded. If failures exceed the threshold (e.g., 5), it switches to Open.</li><li><strong>Open</strong>: Calls are blocked, and a <em>fallback</em> is used. After a set time (e.g., 30 seconds), it moves to Half-Open.</li><li><strong>Half-Open</strong>: A test call is made. Success → Closed, Failure → Open.</li></ol><h3>Simple Code Example</h3><p>Here’s a basic implementation in JavaScript:</p><div><pre><code>class CircuitBreaker {\n&nbsp; constructor(maxFailures = 5, resetTimeout = 30000) {\n&nbsp; &nbsp; this.state = \"CLOSED\";\n&nbsp; &nbsp; this.failureCount = 0;\n&nbsp; &nbsp; this.maxFailures = maxFailures;\n&nbsp; &nbsp; this.resetTimeout = resetTimeout;\n&nbsp; }\n\n\n&nbsp; async call(service) {\n&nbsp; &nbsp; if (this.state === \"OPEN\") {\n&nbsp; &nbsp; &nbsp; if (Date.now() &gt; this.resetTime) {\n&nbsp; &nbsp; &nbsp; &nbsp; this.state = \"HALF_OPEN\";\n&nbsp; &nbsp; &nbsp; } else {\n&nbsp; &nbsp; &nbsp; &nbsp; return \"Fallback: Service unavailable\";\n&nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; }\n\n\n&nbsp; &nbsp; try {\n&nbsp; &nbsp; &nbsp; const result = await service();\n&nbsp; &nbsp; &nbsp; if (this.state === \"HALF_OPEN\") {\n&nbsp; &nbsp; &nbsp; &nbsp; this.state = \"CLOSED\";\n&nbsp; &nbsp; &nbsp; &nbsp; this.failureCount = 0;\n&nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; return result;\n&nbsp; &nbsp; } catch (error) {\n&nbsp; &nbsp; &nbsp; this.failureCount++;\n&nbsp; &nbsp; &nbsp; if (this.failureCount &gt;= this.maxFailures) {\n&nbsp; &nbsp; &nbsp; &nbsp; this.state = \"OPEN\";\n&nbsp; &nbsp; &nbsp; &nbsp; this.resetTime = Date.now() + this.resetTimeout;\n&nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; return \"Fallback: Service unavailable\";\n&nbsp; &nbsp; }\n&nbsp; }\n}\n\n\n// Example usage\nconst breaker = new CircuitBreaker();\nconst fakeService = () =&gt; Math.random() &gt; 0.5 ? \"Success\" : Promise.reject(\"Error\");\nbreaker.call(fakeService).then(console.log);\n</code></pre></div><h2>Circuit Breakers in Serverless</h2><p>In a <em>serverless</em> environment (e.g., AWS Lambda), <em>circuit breakers</em> are still valuable, but their stateless nature poses challenges. The state must be stored externally, such as in DynamoDB.</p><h3>Example in AWS Lambda</h3><div><pre><code>const AWS = require('aws-sdk');\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\n\n\nasync function handler(event) {\n&nbsp; const serviceName = \"ExternalAPI\";\n&nbsp; const state = await dynamodb.get({\n&nbsp; &nbsp; TableName: \"CircuitBreakerState\",\n&nbsp; &nbsp; Key: { Service: serviceName }\n&nbsp; }).promise();\n\n\n&nbsp; if (state.Item?.State === \"OPEN\" &amp;&amp; Date.now() &lt; state.Item.ResetTime) {\n&nbsp; &nbsp; return { statusCode: 503, body: \"Service unavailable\" };\n&nbsp; }\n\n\n&nbsp; try {\n&nbsp; &nbsp; const response = await callExternalAPI();\n&nbsp; &nbsp; if (state.Item?.State === \"HALF_OPEN\") {\n&nbsp; &nbsp; &nbsp; await dynamodb.update({\n&nbsp; &nbsp; &nbsp; &nbsp; TableName: \"CircuitBreakerState\",\n&nbsp; &nbsp; &nbsp; &nbsp; Key: { Service: serviceName },\n&nbsp; &nbsp; &nbsp; &nbsp; UpdateExpression: \"SET #state = :closed\",\n&nbsp; &nbsp; &nbsp; &nbsp; ExpressionAttributeNames: { \"#state\": \"State\" },\n&nbsp; &nbsp; &nbsp; &nbsp; ExpressionAttributeValues: { \":closed\": \"CLOSED\" }\n&nbsp; &nbsp; &nbsp; }).promise();\n&nbsp; &nbsp; }\n&nbsp; &nbsp; return { statusCode: 200, body: response };\n&nbsp; } catch (error) {\n&nbsp; &nbsp; // Logic to update failure count and switch to Open\n&nbsp; &nbsp; return { statusCode: 503, body: \"Service unavailable\" };\n&nbsp; }\n}\n</code></pre></div><h2>Conclusion</h2><p><em>Circuit breakers</em> are a powerful pattern for building resilient systems, whether on traditional servers or in <em>serverless</em> environments. With the simulations and code above, I hope you’ve gained a clearer understanding of how they work.</p>",
    "image": "https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741948558177_circuit_breaker.png",
    "image_alt": "Circuit breaker",
    "slug": "Understanding-Circuit-Breakers-in-Software-Engineering-From-Traditional-to-Serverless"
  },
  {
    "blog_id": "19882a74-d1c2-4b31-837e-99cdc1846fcf",
    "title": "Apache Cassandra: The NoSQL Powerhouse",
    "short_description": "In today's world of big data, scalability and performance are crucial. Apache Cassandra, an open-source NoSQL database, is a top choice for handling large-scale, distributed data. Used by giants like Facebook, Netflix, and Twitter, Cassandra offers high availability, fault tolerance, and seamless scalability. Let’s dive into its architecture and key concepts!",
    "timestamp": "2025-03-14 09:26:37",
    "description": "<h2>Why Choose Apache Cassandra?</h2><p>Unlike traditional relational databases, Cassandra is optimized for handling large workloads across distributed environments. Here’s why it stands out:</p><ul><li><strong>High Availability</strong>: With no single point of failure, Cassandra ensures continuous uptime.</li><li><strong>Horizontal Scalability</strong>: Easily scale out by adding more nodes, avoiding the limitations of vertical scaling.</li><li><strong>Fault Tolerance</strong>: Data replication across nodes guarantees resilience even in case of hardware failures.</li><li><strong>Optimized for Write Operations</strong>: Handles high-speed writes efficiently while offering reliable read performance.</li><li><strong>Flexible Schema</strong>: Unlike relational databases, Cassandra allows schema evolution without downtime.</li></ul><h2>Key Architecture Components</h2><h3>1. <strong>Nodes, Clusters, and Data Centers</strong></h3><ul><li><strong>Node</strong>: The fundamental unit storing a portion of the data.</li><li><strong>Cluster</strong>: A network of nodes working together as a single system.</li><li><strong>Data Center</strong>: A logical grouping of nodes, often used to enhance redundancy across geographical regions.</li></ul><h3>2. <strong>Partitioning &amp; Token Ring</strong></h3><p>Cassandra distributes data across nodes using a <strong>partitioning strategy</strong>, ensuring efficient load balancing. Each node is assigned a <strong>token range</strong>, and data is evenly distributed in a <strong>ring-based architecture</strong>.</p><h3>3. <strong>Replication &amp; Consistency</strong></h3><p>To ensure data availability and reliability, Cassandra employs <strong>replication</strong>:</p><ul><li><strong>Replication Factor (RF)</strong>: Defines the number of copies of data stored across nodes.</li><li><strong>Consistency Levels</strong>: Controls how many nodes must acknowledge a read/write operation (e.g., ONE, QUORUM, ALL), allowing applications to balance performance and reliability.</li></ul><h3>4. <strong>Storage Engine: Commit Log &amp; SSTables</strong></h3><ul><li><strong>Commit Log</strong>: A write-ahead log that captures every write operation for durability before data is flushed to disk.</li><li><strong>Memtable</strong>: A temporary in-memory data structure where writes are stored before being persisted to SSTables.</li><li><strong>SSTables (Sorted String Tables)</strong>: Immutable, append-only files storing actual data on disk, ensuring efficient retrieval and compaction.</li><li><strong>Compaction</strong>: The process of merging multiple SSTables to optimize read performance and free up disk space.</li></ul><h3>5. <strong>Gossip Protocol &amp; Failure Detection</strong></h3><p>Cassandra nodes communicate using the <strong>Gossip Protocol</strong>, a peer-to-peer mechanism for state-sharing, failure detection, and decentralized management.</p><ul><li>Each node periodically exchanges state information with a subset of other nodes.</li><li>Helps maintain a decentralized and resilient system by enabling automatic failure recovery.</li></ul><h3>6. <strong>Read &amp; Write Path in Cassandra</strong></h3><h4><strong>Write Path:</strong></h4><ol><li>Data is written to the <strong>Commit Log</strong> for durability.</li><li>The data is then stored in a <strong>Memtable</strong> (in-memory structure).</li><li>Once the Memtable reaches its threshold, data is flushed to <strong>SSTables</strong> on disk.</li><li>Periodic <strong>compaction</strong> optimizes storage by merging SSTables.</li></ol><h4><strong>Read Path:</strong></h4><ol><li>Cassandra checks the <strong>Memtable</strong> for the latest data.</li><li>If not found, it queries <strong>Bloom Filters</strong> to identify relevant SSTables.</li><li>Reads data from SSTables and merges results before returning them to the client.</li></ol><h2>How Data is Stored &amp; Queried</h2><h3><strong>Primary Keys &amp; Partitions</strong></h3><p>Cassandra structures data into <strong>tables</strong>, similar to relational databases, but with more flexibility. Each table relies on a <strong>Primary Key</strong>, which consists of:</p><ul><li><strong>Partition Key</strong>: Determines data distribution across nodes.</li><li><strong>Clustering Key</strong>: Defines the sorting order of data within a partition.</li></ul><h3><strong>Querying with CQL (Cassandra Query Language)</strong></h3><p>Cassandra utilizes CQL, a SQL-like query language tailored for distributed storage.</p><h4>Example Table Creation:</h4><div><pre><code>CREATE TABLE users (\n  id UUID PRIMARY KEY,\n  name TEXT,\n  email TEXT,\n  age INT\n);\n</code></pre></div><p>However, to maintain speed and efficiency, Cassandra does not support SQL-like JOINs and complex ACID transactions.</p><h2>When to Use Cassandra?</h2><h3><strong>Best Use Cases:</strong></h3><ul><li>Applications requiring <strong>high availability</strong> (e.g., messaging apps, IoT data processing, recommendation engines)</li><li>Large-scale <strong>real-time analytics</strong></li><li><strong>Distributed content delivery</strong> systems</li><li><strong>Financial services</strong> handling time-series data</li></ul><h3><strong>Not Ideal For:</strong></h3><ul><li>Complex transactional applications requiring <strong>strict ACID compliance</strong></li><li>Applications needing frequent <strong>JOIN operations</strong> and deep relational modeling</li></ul><h2>Conclusion</h2><p>Apache Cassandra is a powerful NoSQL database designed for organizations that need to manage high-velocity, large-scale data efficiently. Its distributed architecture, fault tolerance, and seamless scalability make it a prime choice for modern applications handling mission-critical workloads. If you're looking for a battle-tested NoSQL solution capable of global-scale operations, Cassandra is worth exploring!</p>",
    "image": "https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741944106846_apache_cassandra.png",
    "image_alt": "Apache Cassandra",
    "slug": "Apache-Cassandra-The-NoSQL-Powerhouse"
  },
  {
    "blog_id": "52eb37c3-83bc-4442-a8c5-305bfba74e62",
    "title": "Why API Versioning is Really Important: A Lesson from My Own Mistake",
    "short_description": "As a developer, I've built countless APIs for my personal projects. Some were experimental, some turned into full-fledged applications, and others were simply abandoned over time. At first, managing these APIs felt simple—if I wasn't using an endpoint anymore, I would just delete it. Why keep something that I no longer need, right?  Well, that mindset came back to bite me.",
    "timestamp": "2025-02-16 13:35:40",
    "description": "<h1><strong>The Mistake That Taught Me a Lesson</strong></h1><p>One day, I was cleaning up an old project, removing unused routes and refactoring the backend. There was this one API endpoint—let's call it /user/details—that I thought was no longer in use. Without a second thought, I deleted it and pushed the changes to production.</p><p>A few hours later, I started receiving errors from another service I had built months earlier. This service, which I had completely forgotten about, was still making requests to /user/details. Suddenly, parts of my application were broken, and I had no easy way to recover from it.</p><p>That was the moment I truly understood why API versioning is critical.</p><h2><strong>Why API Versioning Matters</strong></h2><p><strong>1. Prevents Breaking Changes</strong></p><p>When APIs evolve, clients relying on them should not break due to changes. By implementing versioning (e.g., /v1/user/details), I could have introduced a new version while keeping the old one intact for existing consumers.</p><p><strong>2. Maintains Backward Compatibility</strong></p><p>Even if you think an API is no longer needed, there’s a chance some service or third-party client is still using it. Versioning allows developers to deprecate old APIs gradually rather than abruptly removing them.</p><p><strong>3. Gives Users Time to Migrate</strong></p><p>If an API must change, users need time to update their applications. Providing multiple versions (e.g., /v1/, /v2/) ensures a smooth transition.</p><p><strong>4. Helps in Debugging and Maintenance</strong></p><p>When multiple versions exist, issues can be traced more easily. If a bug appears in /v2/ but not in /v1/, it’s easier to identify what changes might have caused it.</p><h2>How to Implement API Versioning</h2><h3>1. <strong>URL Versioning</strong></h3><p>The most common and widely adopted approach to API versioning is using version numbers in the URL.</p><div><pre><code>/v1/users\n/v2/users\n</code></pre></div><h4>Pros:</h4><ul><li><strong>Easy to understand and implement</strong> – Developers can quickly identify which version is being used.</li><li><strong>Clear distinction between versions</strong> – Each version has its own endpoint, ensuring that changes do not interfere with older versions.</li></ul><h4>Cons:</h4><ul><li><strong>Can lead to bloated URLs</strong> – If too many versions exist, the API can become cluttered.</li><li><strong>Might require modifying routes and maintaining multiple endpoints</strong> – Developers must maintain multiple versions, which can increase complexity over time.</li></ul><h3>2. <strong>Header Versioning</strong></h3><p>Another approach is to use HTTP headers to specify the API version instead of embedding it in the URL.</p><div><pre><code>Accept: application/vnd.myapi.v1+json\n</code></pre></div><h4>Pros:</h4><ul><li><strong>Keeps URLs clean</strong> – There’s no need to modify the URL structure, making it aesthetically cleaner.</li><li><strong>Allows more flexibility without changing routes</strong> – Clients can request different versions dynamically using headers.</li></ul><h4>Cons:</h4><ul><li><strong>Requires clients to send custom headers explicitly</strong> – Clients must be aware of the correct headers to use, which adds complexity.</li><li><strong>Might be harder to test and debug compared to URL versioning</strong> – Since versioning is not visible in the URL, debugging and API documentation can be more challenging.</li></ul><h3>3. <strong>Query Parameter Versioning</strong></h3><p>This method involves specifying the API version as a query parameter in the request.</p><div><pre><code>/users?version=1\n</code></pre></div><h4>Pros:</h4><ul><li><strong>Simple to implement and does not require changes to routes</strong> – The backend can handle different versions without modifying the API structure.</li><li><strong>Can be easily handled on the backend</strong> – Developers can dynamically parse the version parameter and route requests accordingly.</li></ul><h4>Cons:</h4><ul><li><strong>Can lead to inconsistent API calls if clients forget to include the version</strong> – If a request is made without the version parameter, it may result in unintended behavior.</li><li><strong>May clutter the query string with additional parameters</strong> – This approach can become cumbersome if multiple parameters are needed.</li></ul><h2><strong>Choosing the Right API Versioning Strategy</strong></h2><p>Each of these methods has its strengths and weaknesses, and the best approach depends on the specific needs of your project. If you want a simple and widely understood method, <strong>URL versioning</strong> might be the best choice. If you prefer a cleaner URL structure, <strong>header versioning</strong> could be a better fit. And if you need quick implementation without altering routes, <strong>query parameter versioning</strong> is a viable option.</p><h2><strong>Final Thoughts</strong></h2><p>I learned the hard way that careless API deletions can lead to unexpected failures. If I had implemented proper versioning, I could have safely iterated on my APIs without breaking my own services.</p><p>So, if you're developing APIs—whether for personal projects or production systems—take API versioning seriously. Your future self (and your users) will thank you!</p>",
    "image": "https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1739712690763_api-versioning-strategy.jpg",
    "image_alt": "Api Versioning",
    "slug": "Why-API-Versioning-is-Really-Important-A-Lesson-from-My-Own-Mistake"
  },
  {
    "blog_id": "911a9001-3c3e-4f2c-aa83-4ec4f6f71c99",
    "title": "Terraform Labs: Automating Google Cloud Infrastructure Deployment",
    "short_description": "Manually managing cloud infrastructure can be time-consuming and error-prone. Terraform changes the game by allowing you to define infrastructure as code, making deployment faster, scalable, and repeatable. With Terraform, you can automate cloud resource creation, track changes, and collaborate effortlessly.",
    "timestamp": "2025-02-16 02:15:22",
    "description": "<h2><strong>How Does Terraform Work?</strong></h2><h3><strong>1. Declarative Configuration</strong></h3><p>Terraform uses a special language called <strong>HashiCorp Configuration Language (HCL)</strong>. Instead of writing step-by-step instructions like in traditional programming, you just <strong>describe what you want</strong> (e.g., \"I need a virtual machine with 2 CPUs and 4GB RAM\"), and Terraform figures out how to make it happen.</p><h3><strong>2. Configuration Interpretation</strong></h3><p>When you run a Terraform command, it reads your configuration files and <strong>understands what infrastructure you want to create or update</strong>.</p><h3><strong>3. Interaction with Cloud APIs</strong></h3><p>Terraform then communicates with cloud providers like <strong>Google Cloud, AWS, or Azure</strong> by sending <strong>API requests</strong>. This tells the cloud provider to create, update, or delete the resources you defined.</p><h3><strong>4. Execution by Cloud Providers</strong></h3><p>The cloud provider takes Terraform’s instructions and <strong>builds your infrastructure</strong>—creating things like virtual machines, networks, and storage based on your configuration.</p><h3><strong>5. State Management</strong></h3><p>Terraform keeps track of everything it has created in a <strong>state file</strong>. This file helps Terraform know what’s already there, so it only makes <strong>necessary changes</strong> when you update your configuration.</p><h3><strong>Why Use Terraform?</strong></h3><p>Terraform makes infrastructure management <strong>simpler, repeatable, and error-free</strong>. Instead of manually clicking around in cloud dashboards, you can <strong>automate everything</strong> with a few lines of code. This saves time and reduces mistakes.</p><p>Terraform enables you to safely and predictably create, change, and improve infrastructure. It is an open-source tool that codifies APIs into declarative configuration files that can be shared among team members, treated as code, edited, reviewed, and versioned.</p><p>In this lab, you create a Terraform configuration with a module to automate the deployment of Google Cloud infrastructure.</p><h2><strong>Task 1: Setting Up Terraform and Cloud Shell</strong></h2><h3><strong>Installing Terraform</strong></h3><p>Terraform is pre-installed in Cloud Shell. Verify the installed version.</p><p>1 Open <strong>Google Cloud Console</strong> and click <strong>Activate Cloud Shell</strong>.</p><p>2 If prompted, click <strong>Continue</strong>.</p><p>3 Run the following command to check the Terraform version:</p><div><pre><code>terraform --version\n</code></pre></div><p><strong>- Expected output:</strong></p><div><pre><code>Terraform v1.3.3\n</code></pre></div><blockquote><strong>Note:</strong> These lab instructions work with Terraform v1.3.3 and later.</blockquote><p>4 Create a directory for Terraform configurations:</p><div><pre><code>mkdir tfinfra\n</code></pre></div><p>5 Open <strong>Cloud Shell Editor</strong> and navigate to the <strong><em>tfinfra </em></strong>folder.</p><h3><strong>Initializing Terraform</strong></h3><p>Terraform uses plugins to support various cloud providers. Initialize Terraform by setting Google as the provider.</p><p>1 Create a new file named <strong>provider.tf</strong> tfinfra folder.</p><p>2 Add the following configuration:</p><div><pre><code>provider \"google\" {}\n</code></pre></div><p>3 Save the file.</p><p>4 Run the Terraform initialization command:</p><div><pre><code>cd tfinfra\nterraform init\n</code></pre></div><p><strong>- Expected output:</strong></p><div><pre><code>provider.google: version = \"~&gt; 4.43.0\"\nTerraform has been successfully initialized!\n</code></pre></div><h2><strong>Task 2: Creating mynetwork and Its Resources</strong></h2><h3><strong>Configuring mynetwork</strong></h3><p>1 Create a new file named <strong>mynetwork.tf</strong> inside tfinfra.</p><p>2 Add the following configuration:</p><div><pre><code>resource \"google_compute_network\" \"mynetwork\" {\n  name                    = \"mynetwork\"\n  auto_create_subnetworks = true\n}\n</code></pre></div><p>3 Save the file.</p><h3><strong>Configuring Firewall Rules</strong></h3><p>4 Add the following firewall rules to mynetwork,tf:</p><div><pre><code>resource \"google_compute_firewall\" \"mynetwork-allow-http-ssh-rdp-icmp\" {\n  name    = \"mynetwork-allow-http-ssh-rdp-icmp\"\n  network = google_compute_network.mynetwork.self_link\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"22\", \"80\", \"3389\"]\n  }\n  allow {\n    protocol = \"icmp\"\n  }\n  source_ranges = [\"0.0.0.0/0\"]\n}\n</code></pre></div><p>5 Save the file.</p><h3><strong>Configuring VM Instance</strong></h3><p>1 Create a new folder named <strong>instance</strong> inside tfinfra.</p><p>2 Create a new file <strong>main.tf</strong> inside the instance folder.</p><p>3 Add the following basic configuration:</p><div><pre><code>resource \"google_compute_instance\" \"vm_instance\" {\n  name         = \"my-vm-instance\"\n  machine_type = \"e2-medium\"\n  zone         = \"us-central1-a\"\n\n  boot_disk {\n    initialize_params {\n      image = \"debian-cloud/debian-10\"\n    }\n  }\n  network_interface {\n    network = google_compute_network.mynetwork.self_link\n  }\n}\n</code></pre></div><p>4 Save the file.</p><p>To rewrite the Terraform configuration files to a canonical format and style, run the following command:</p><div><pre><code>terraform fmt\n</code></pre></div><p>To initialize Terraform, run the following command:</p><div><pre><code>terraform init\n</code></pre></div><p><strong>Expected output:</strong></p><div><pre><code>...\n* provider.google: version = \"~&gt; 4.43.0\"\n\nTerraform has been successfully initialized!\n</code></pre></div><h2><strong>Conclusion</strong></h2><p>You have successfully set up Terraform in Cloud Shell and created configurations to deploy Google Cloud infrastructure, including a VPC network, firewall rules, and VM instances. Terraform’s execution workflow ensures smooth infrastructure deployment with minimal manual intervention. This setup can be expanded with additional configurations and modules to efficiently automate more complex infrastructure deployments.</p><p>Happy learning and coding with Terraform!</p>",
    "image": "https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1739669992396_gcp-terraform.png",
    "image_alt": "Terraform with GCP",
    "slug": "Terraform-Labs-Automating-Google-Cloud-Infrastructure-Deployment"
  },
  {
    "blog_id": "109a123a-02ae-4b9f-96a9-785428eef2fa",
    "title": "Jenkins Unleashed: Transforming Your CI/CD Workflow for Lightning-Fast Delivery",
    "short_description": "In the fast-paced world of modern software development, delivering high-quality applications quickly is no longer optional—it's essential. This is where Jenkins steps in as a game-changer. Imagine having a virtual assistant that tirelessly builds, tests, and deploys your code, ensuring every update you make reaches production seamlessly.",
    "timestamp": "2025-01-06 12:32:13",
    "description": "<h1><strong>Why Use Jenkins for CI/CD?</strong></h1><p>Jenkins is not just a tool; it's a culture shifter. With its unparalleled flexibility, thousands of plugins, and vibrant community, Jenkins transforms how teams approach Continuous Integration and Continuous Delivery (CI/CD). Whether you’re a small startup racing to push your MVP or a large enterprise managing complex workflows, Jenkins empowers you to automate repetitive tasks, reduce errors, and accelerate delivery pipelines.</p><p>But why Jenkins? It’s open-source, highly customizable, and scales effortlessly with your team’s growing needs. It’s the bridge between developers and operations teams, breaking down silos and fostering collaboration in ways you’ve never experienced before.</p><p>In this labs, we’ll explore the magic of Jenkins and why it’s the go-to choice for CI/CD pipelines. Ready to revolutionize your development workflow? Let’s dive in!</p><h1><strong>Setting Up CI/CD in Jenkins</strong></h1><p>To start setting up CI/CD with Jenkins, the first step is to prepare your environment by installing Java. Jenkins requires Java to run, so you need to install OpenJDK 17. Update your system packages and install Java by running the following commands:</p><div><pre><code>sudo apt update\nsudo apt install fontconfig openjdk-17-jre\njava -version\n</code></pre></div><p>After installation, you can verify the version of Java to ensure everything is properly set up. The output should display the installed version, such as openJDK version 17. With Java in place, your system is now ready to host Jenkins.</p><p>Next, install Jenkins using its official package repository to ensure you're getting the latest stable release. First, add Jenkins' repository key and configuration to your system, update the package list, and install Jenkins by running these commands:</p><div><pre><code>sudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\\n  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key\necho \"deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]\" \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install jenkins\n</code></pre></div><p>Once Jenkins is installed, you need to enable and start the Jenkins service. This ensures Jenkins runs immediately and starts automatically whenever the system boots. Use the following commands to enable and start Jenkins:</p><div><pre><code>sudo systemctl enable jenkins\nsudo systemctl start jenkins\nsudo systemctl status jenkins\n</code></pre></div><p>After starting the service, you can access Jenkins through your browser. Open http://[VM_EXTERNAL_IP]:8080&nbsp;and log in using the initial admin password. To retrieve this password, run:</p><div><pre><code>sudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre></div><p>Enter the password on the setup screen and follow the guided setup wizard. This process will include installing recommended plugins, such as Git, Pipeline, Blue Ocean, and GitHub, which are essential for building your CI/CD pipeline.</p><p>In addition to plugins, configure credentials for accessing your source code repository. Navigate to <strong>Manage Jenkins &gt; Credentials</strong>, and add secure credentials for platforms like GitHub or GitLab. This setup ensures your pipeline can pull code from your repositories securely.</p><p>With Jenkins configured, you’re ready to create your first pipeline. The pipeline script can define all stages of your CI/CD process, such as fetching code, running tests, and deploying to production.</p><h1><strong>Setting Up Git Credentials for Private Repositories</strong></h1><p>If your project is hosted on a private Git platform, you’ll need to provide secure credentials for Jenkins to access the repository. For GitHub users, this involves creating a Personal Access Token (PAT) and adding it to Jenkins' credentials. This ensures seamless integration between Jenkins and your repository without compromising security.</p><h2><strong>Creating a Personal Access Token (PAT) on GitHub</strong></h2><p>To create a PAT in GitHub:</p><p>1. Go to your GitHub account settings.</p><p>2. Navigate to Developer settings &gt; Personal Access Tokens &gt; Tokens (classic).</p><p>3. Click Generate new token and specify the required permissions. For most CI/CD setups, select scopes like repo (for repository access) and workflow (if managing GitHub Actions).</p><p>4. Generate the token and copy it. Make sure to store it securely as you won’t be able to view it again.</p><h2>Configuring Jenkins with GitHub Credentials</h2><p>Once you’ve created your PAT, follow these steps to add it to Jenkins:</p><p>1. Log in to the Jenkins Dashboard.</p><p>2. Navigate to Manage Jenkins &gt; Manage Credentials.</p><p>3. Under (global) credentials, click Add Credentials.</p><p>4. In the Kind dropdown, select Username with password.</p><p>Username: Enter your GitHub username (e.g., username).</p><p>Password: Paste the PAT you just created.</p><p>5. Give the credential a recognizable ID (e.g., github-creds) and click OK.</p><h1>Writing Your First Jenkins Pipeline</h1><p>With Jenkins configured, you’re ready to create your first pipeline. The pipeline script defines all stages of your CI/CD process, including fetching code, running tests, and deploying the application. Let’s walk through an example pipeline written using Jenkins' declarative syntax, which simplifies the process and ensures better readability.</p><h4>Understanding the Pipeline Script</h4><p>Below is an example pipeline that automates three key stages of the CI/CD process:</p><p><strong>1. Fetching the code from GitHub</strong> using credentials for a secure connection.</p><p><strong>2. Running basic testing commands</strong> to verify the environment setup.</p><p><strong>3. Deploying the application</strong> to a server using SSH for remote commands</p><div><pre><code>pipeline {\n&nbsp; &nbsp; agent any\n&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; environment {\n&nbsp; &nbsp; &nbsp; &nbsp; GITHUB_CREDENTIALS = 'caea020d-a24e-4305-bdc2-d7e51d1c8171'&nbsp; // ID of the GitHub credentials in Jenkins\n&nbsp; &nbsp; }\n&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; stages {\n&nbsp; &nbsp; &nbsp; &nbsp; stage('Git Checkout') {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; steps {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; script {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Cloning the GitHub repository using the provided credentials\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; git credentialsId: \"${GITHUB_CREDENTIALS}\", url: 'https://github.com/Barbarpotato/API-Registry.git', branch: 'main'\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; stage('Run Testing Commands') {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; steps {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sh 'hostname'&nbsp; // Outputs the hostname of the Jenkins agent\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sh 'pwd'&nbsp; &nbsp; &nbsp; &nbsp;// Displays the current working directory in the agent's workspace\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; stage('Deploy to Server') {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; steps {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Using SSH to deploy the application to the target server\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sshagent(['ssh-key-gateway']) {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sh '''\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ssh -o StrictHostKeyChecking=no darmawanjr88@34.101.205.217 &lt;&lt; EOF\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; cd API-Registry\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; git pull origin main\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; pm2 restart all\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; exit\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; EOF\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; '''\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; }\n}\n</code></pre></div><p>The provided pipeline script is a clear example of how Jenkins orchestrates the CI/CD process, broken into multiple stages for ease of management. Let's break it down section by section and explain how each part works in a simplified, interactive way.</p><h3>Pipeline Declaration</h3><p>At the heart of any Jenkins pipeline is the pipeline block, which defines the entire process. Inside this block, the agent any directive tells Jenkins to run the pipeline on any available agent, whether it's a master or a worker node. This flexibility is useful when you have multiple agents configured and don’t want to restrict the execution to a specific one.</p><h3>Environment Variables</h3><p>Next, we have the environment block. This is where we can define variables that are reused across the pipeline. In this case, the variable GITHUB_CREDENTIALS holds the ID of the GitHub credentials that Jenkins uses to securely access the private repository. By defining it here, you ensure it’s easily reusable without hardcoding it into every step. Think of it as a centralized way to manage sensitive data like tokens and credentials, making the pipeline both secure and maintainable.</p><h3>Stages</h3><p>The pipeline is broken into logical steps called \"stages,\" each representing a part of the CI/CD workflow.</p><p>1. Git Checkout</p><p>In the first stage, Jenkins clones the GitHub repository using the git step. The credentialsId points to the pre-configured GitHub credentials stored in Jenkins. This ensures secure and seamless access to the private repository without exposing sensitive information. This step lays the foundation for the entire pipeline, as it fetches the code that the remaining stages will process.</p><p>2. Run Testing Commands</p><p>This stage is simple but powerful. It runs shell commands such as hostname and pwd, which output the hostname of the Jenkins agent and the current working directory, respectively. While these commands are placeholders here, you can replace them with actual test scripts. For instance, if you’re running unit tests, you could include a command like npm test or pytest. The purpose of this stage is to ensure the environment is configured correctly and ready for further operations.</p><p>3. Deploy to Server</p><p>This stage demonstrates how Jenkins can deploy your application to a production or staging server. Using the sshagent block, Jenkins securely connects to the target server via SSH. It then pulls the latest changes from the repository and restarts the application using PM2, a popular process manager for Node.js. This setup ensures that the application is always up-to-date with the latest code changes, and the restart command ensures a smooth rollout of updates.</p><p>Continuing from the previous explanation of setting up a Jenkins pipeline, the next step to truly automate the CI/CD process is to set up a <strong>push trigger</strong> using webhooks. This ensures that every time you push changes to your GitHub repository, Jenkins automatically triggers the pipeline, saving you from the hassle of manually starting the build.</p><p>Let’s explore how to set up a webhook-based trigger between GitHub and Jenkins in an intuitive and straightforward way.</p><h3><strong>What Are Webhooks?</strong></h3><p>Think of webhooks as a way for GitHub to \"talk\" to Jenkins. Whenever you push code to your repository, GitHub sends a signal (HTTP POST request) to Jenkins, telling it to start the pipeline. This creates an automated, real-time link between your code changes and the build process.</p><h3><strong>Setting Up Push Trigger (Webhook)</strong></h3><h4><strong>First, Enable GitHub Integration in Jenkins</strong></h4><p>Before setting up the webhook, you need to make sure Jenkins can communicate with GitHub. To do this, open Jenkins and go to <strong>Manage Jenkins</strong> &gt; <strong>Manage Plugins</strong>. From there, search for <strong>GitHub</strong> plugins and install them. This will allow Jenkins to recognize GitHub as a source and receive notifications from it.</p><h4><strong>Configure Jenkins to Listen for Webhooks</strong></h4><p>Next, open the Jenkins pipeline job you want to configure. In the job settings, go to <strong>Build Triggers</strong> and enable the option <strong>GitHub hook trigger for GITScm polling</strong>. This step is important because it tells Jenkins to \"listen\" for any push events from GitHub, ready to trigger the pipeline whenever changes are detected.</p><h4><strong>Set Up a Webhook in GitHub</strong></h4><p>Now, head over to your GitHub repository. Inside the <strong>Settings</strong> tab, navigate to <strong>Webhooks</strong> and click on <strong>Add webhook</strong>. You'll need to provide Jenkins with a specific endpoint where it can receive notifications from GitHub. The URL format is as follows:</p><div><pre><code>http://&lt;JENKINS_URL&gt;/github-webhook/\n</code></pre></div><p>Replace <strong><em>JENKINS_URL </em></strong>with your Jenkins server’s address. Choose <strong>application/json</strong> as the content type, and make sure the <strong>Push events</strong> option is selected. This ensures that the webhook triggers every time you push changes.</p><h4><strong>Verify the Webhook</strong></h4><p>Once everything is set up, push a commit to your GitHub repository. Then, return to Jenkins and check if the pipeline starts running automatically. If the job kicks off, the webhook is working properly.</p><h3><strong>How It All Comes Together</strong></h3><p>When you push changes to your repository, GitHub sends a webhook to Jenkins. Jenkins then triggers the pipeline to fetch the latest code, run tests, and deploy to your server. This creates a seamless CI/CD process, where every change is automatically tested and deployed.</p><h1><strong>Conclusion</strong></h1><p>In this Labs, we’ve taken a deep dive into how Jenkins can supercharge your CI/CD workflows, making it an essential tool for automating your development lifecycle. We started by setting up Jenkins, installing necessary dependencies like Java, and getting the Jenkins service up and running.</p><p>From there, we explored how to configure Jenkins to work with your GitHub repository, including managing credentials securely. With Jenkins set up and connected to GitHub, we moved on to creating a simple declarative pipeline, allowing Jenkins to automatically fetch the latest code, run tests, and deploy to your server.</p><p>We then enhanced the process by explaining how to set up push triggers using GitHub webhooks. With webhooks in place, Jenkins is able to automatically start the pipeline whenever new code is pushed to the repository, eliminating the need for manual intervention and ensuring continuous integration.</p><p>Through these steps, we’ve created a fully automated CI/CD pipeline that reacts to changes in your code, testing it and deploying it seamlessly. This not only saves time but also minimizes errors, providing faster and more reliable software delivery.</p><p>By mastering Jenkins, you’re empowering yourself to automate complex workflows, improve collaboration, and focus on building great software without worrying about the manual process of integration and deployment.</p>",
    "image": "https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fjenkins_background.png?alt=media&token=8d8f21c7-f6bd-4157-8343-12090e88d13a",
    "image_alt": "Jenkins Intro image",
    "slug": "Jenkins-Unleashed-Transforming-Your-CICD-Workflow-for-Lightning-Fast-Delivery"
  }
]